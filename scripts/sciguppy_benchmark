#!/usr/bin/env python

import numpy
import math
import click
import timeit
import scipy.signal
import scipy.special
import sciguppy

a = numpy.random.rand(100,100,100).astype(numpy.float32)
x = numpy.random.rand(100,100,5).astype(numpy.float32)
y = numpy.random.rand(4,20,3).astype(numpy.float32)

i = numpy.random.rand(100000, 50).astype(numpy.float32)
j = numpy.random.rand(50, 1).astype(numpy.float32)

def correlate_base(a1, a2, mode=sciguppy.CorrelationModes.FULL):
    return scipy.signal.correlate(a1, a2, mode=mode.value)

@click.group()
@click.option('--mode', default=sciguppy.CorrelationModes.FULL.name, type=click.Choice([sciguppy.CorrelationModes.VALID.name, sciguppy.CorrelationModes.FULL.name]))
@click.option('--memcpy/--no-memcpy', default=True)
@click.option('--fast/--no-fast', default=False)
@click.pass_context
def benchmark(ctx, mode, memcpy, fast):
    ctx.obj['mode'] = getattr(sciguppy.CorrelationModes, mode)
    ctx.obj['memcpy'] = memcpy
    ctx.obj['fast'] = fast 

@benchmark.command()
@click.pass_context
def test(ctx):
    out1 = correlate_base(x, y, mode=sciguppy.CorrelationModes.FULL)
    out2 = sciguppy.correlate(x, y, mode=sciguppy.CorrelationModes.FULL)
    assert numpy.allclose(out1, out2)

    out1 = correlate_base(x, y, mode=sciguppy.CorrelationModes.VALID)
    out2 = sciguppy.correlate(x, y, mode=sciguppy.CorrelationModes.VALID)
    assert numpy.allclose(out1, out2)

    out1 = scipy.special.expit(a)
    out2 = sciguppy.expit(a)
    out3 = sciguppy.expit(a, mode=sciguppy.MathModes.FAST)
    assert numpy.allclose(out1, out2)
    assert numpy.allclose(out1, out3)

    out1 = numpy.dot(i, j)
    out2 = sciguppy.dot(i, j)
    assert numpy.allclose(out1, out2)

@benchmark.command()
@click.pass_context
def all(ctx):
    mode = ctx.obj['mode']
    correlate_cpu = lambda: correlate_base(x, y, mode=mode)
    if ctx.obj['memcpy']:
        correlate_gpu = lambda: sciguppy.correlate(x, y, mode=mode)
    else:
        d_x = sciguppy.utils.as_gpu(x)
        d_y = sciguppy.utils.as_gpu(y)
        correlate_gpu = lambda: sciguppy.correlate(d_x, d_y, mode=mode, return_type=sciguppy.ArrayReturnTypes.GPU)
    run_benchmark('correlate', correlate_cpu, correlate_gpu)

    mode = sciguppy.MathModes.FAST if ctx.obj['fast'] else sciguppy.MathModes.ACC
    expit_cpu = lambda: scipy.special.expit(a)
    if ctx.obj['memcpy']:
        expit_gpu = lambda: sciguppy.expit(a, mode=mode)
    else:
        d_a = sciguppy.utils.as_gpu(a)
        expit_gpu = lambda: sciguppy.expit(d_a, mode=mode, return_type=sciguppy.ArrayReturnTypes.GPU)
    run_benchmark('expit', expit_cpu, expit_gpu)

    cpu = lambda: numpy.dot(i, j)
    if ctx.obj['memcpy']:
        gpu = lambda: sciguppy.dot(i, j)
    else:
        d_i = sciguppy.utils.as_gpu(i)
        d_j = sciguppy.utils.as_gpu(j)
        gpu = lambda: sciguppy.dot(d_i, d_j, return_type=sciguppy.ArrayReturnTypes.GPU)
    run_benchmark('dot', cpu, gpu)

@benchmark.command()
@click.pass_context
def gpu(ctx):
    sciguppy.correlate(x, y, mode=ctx.obj['mode'])

def run_benchmark(name, cpu_func, gpu_func):
    print name
    baseline = timeit.repeat(cpu_func, repeat=10, number=1)

    gpu_func() # warm the gpu before benchmarking
    gpu_baseline = timeit.repeat(gpu_func, repeat=10, number=1)

    print '\tbaseline: avg={}, min={}, max={}'.format(numpy.average(baseline), min(baseline), max(baseline))
    print '\tgpu baseline: avg={}, min={}, max={}'.format(numpy.average(gpu_baseline), min(gpu_baseline), max(gpu_baseline))

if __name__ == '__main__':
    benchmark(obj={})
